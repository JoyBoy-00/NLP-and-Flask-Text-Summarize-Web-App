{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF2rTKJQM+82aJCLk9lvAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyBoy-00/NLP-and-Flask-Text-Summarize-Web-App/blob/main/modeltextsummary1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYDZ42jURm7A",
        "outputId": "8bc6ecf6-db89-4173-f9e6-d32c15611081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (317 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 125044 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.2:1b\n",
        "clear_output()\n",
        "!pip install -U lightrag[ollama]\n",
        "from IPython.display import clear_output, Markdown, display\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Initialize SBERT model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the QA template\n",
        "qa_template = r\"\"\"\n",
        "You are a helpful assistant.\n",
        "\n",
        "User: {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "class SBERTQA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        # Generate SBERT embeddings for the input text\n",
        "        input_text = str(input)\n",
        "        embeddings = self.sbert_model.encode(input_text, convert_to_tensor=True)\n",
        "\n",
        "        # Use the embeddings to generate a summary\n",
        "        summary = self.generator.call({\"input_str\": input_text})\n",
        "        return summary\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        # Generate SBERT embeddings for the input text\n",
        "        input_text = str(input)\n",
        "        embeddings = self.sbert_model.encode(input_text, convert_to_tensor=True)\n",
        "\n",
        "        # Use the embeddings to generate a summary\n",
        "        summary = await self.generator.acall({\"input_str\": input_text})\n",
        "        return summary\n",
        "\n",
        "# Initialize the model\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.2:1b\"}\n",
        "}\n",
        "\n",
        "# Create the SBERTQA component\n",
        "qa = SBERTQA(**model)\n",
        "\n",
        "# Generate a summary\n",
        "input_text = \"Summarize in 200 words What is the meaning of life?\"\n",
        "output = qa.call(input_text)\n",
        "\n",
        "# Display the summary\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "9kpuTyTRRtH-",
        "outputId": "e2428552-34e4-4fe3-8579-658f57a2ff98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightrag[ollama] in /usr/local/lib/python3.11/dist-packages (0.1.0b6)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (3.1.6)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (4.0.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.26.4)\n",
            "Requirement already satisfied: ollama<0.3.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (0.2.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (3.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (25.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.3.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** The question of the meaning of life is one of the most profound and complex questions humanity has ever pondered. There is no straightforward answer, as it depends on individual perspectives, beliefs, and values. However, various philosophical, religious, and cultural theories attempt to provide insights.\n\nSome possible interpretations include:\n\n- Existentialism: The idea that life has no inherent meaning, but individuals must create their own purpose through choices and actions.\n- Stoicism: The pursuit of virtue, self-control, and inner strength as a means to find meaning in life.\n- Optimism: Believing that life's meaning can be discovered through personal growth, relationships, and achievements.\n- Nihilism: The rejection of all inherent meaning or value, leading individuals to focus on individual freedom and choice.\n\nUltimately, the meaning of life may be unique to each person, shaped by their experiences, emotions, and perspectives. While some find purpose in relationships, work, or personal growth, others may derive meaning from spirituality, art, or a sense of transcendence. The search for meaning is a lifelong journey, with no definitive answer, but rather a continuous exploration of one's values and passions."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: clone repo in github\n",
        "\n",
        "!git clone https://github.com/username/repository.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3h28FuusjxN",
        "outputId": "3d02b6e1-407c-489d-980b-7acabb6a2892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-and-Flask-Text-Summarize-Web-App'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9D62bbqoscwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}